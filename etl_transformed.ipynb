{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bf320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files created in data/ folder\n",
      "Transformation 1: Handling missing values\n",
      "\n",
      "Before Cleaning - Missing Values:\n",
      "   order_id customer_name product  quantity  unit_price order_date region\n",
      "0         1         Diana  Tablet       NaN       500.0  1/20/2024  South\n",
      "1         2           Eve  Laptop       NaN         NaN  4/29/2024  North\n",
      "2         3       Charlie  Laptop       2.0       250.0   1/8/2024    NaN\n",
      "3         4           Eve  Laptop       2.0       750.0   1/7/2024   West\n",
      "4         5           Eve  Tablet       3.0         NaN   3/7/2024  South\n",
      "\n",
      "After Cleaning - Missing Values:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region\n",
      "0         1         Diana  Tablet       2.0       500.0  1/20/2024    South\n",
      "1         2           Eve  Laptop       2.0       750.0  4/29/2024    North\n",
      "2         3       Charlie  Laptop       2.0       250.0   1/8/2024  Unknown\n",
      "3         4           Eve  Laptop       2.0       750.0   1/7/2024     West\n",
      "4         5           Eve  Tablet       3.0       750.0   3/7/2024    South\n",
      "\n",
      "Before Cleaning - Missing Values:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region\n",
      "0       101         Alice  Laptop       NaN       900.0   5/9/2024  Central\n",
      "1       102           NaN  Laptop       1.0       300.0   5/7/2024  Central\n",
      "2       103           NaN  Laptop       1.0       600.0   5/4/2024  Central\n",
      "3       104           NaN  Tablet       NaN       300.0  5/26/2024  Central\n",
      "4       105         Heidi  Tablet       2.0       600.0  5/21/2024    North\n",
      "\n",
      "After Cleaning - Missing Values:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region\n",
      "0       101         Alice  Laptop       1.5       900.0   5/9/2024  Central\n",
      "1       102       Unknown  Laptop       1.0       300.0   5/7/2024  Central\n",
      "2       103       Unknown  Laptop       1.0       600.0   5/4/2024  Central\n",
      "3       104       Unknown  Tablet       1.5       300.0  5/26/2024  Central\n",
      "4       105         Heidi  Tablet       2.0       600.0  5/21/2024    North\n",
      "Why: Missing values can skew analysis. Imputed customer_name and region with 'Unknown', quantity and unit_price with median, and order_date with a default date to maintain data integrity.\n",
      "\n",
      "Transformation 2: Removing duplicates\n",
      "\n",
      "Before Removing Duplicates:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region\n",
      "0         1         Diana  Tablet       2.0       500.0  1/20/2024    South\n",
      "1         2           Eve  Laptop       2.0       750.0  4/29/2024    North\n",
      "2         3       Charlie  Laptop       2.0       250.0   1/8/2024  Unknown\n",
      "3         4           Eve  Laptop       2.0       750.0   1/7/2024     West\n",
      "4         5           Eve  Tablet       3.0       750.0   3/7/2024    South\n",
      "\n",
      "After Removing Duplicates:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region\n",
      "0         1         Diana  Tablet       2.0       500.0  1/20/2024    South\n",
      "1         2           Eve  Laptop       2.0       750.0  4/29/2024    North\n",
      "2         3       Charlie  Laptop       2.0       250.0   1/8/2024  Unknown\n",
      "3         4           Eve  Laptop       2.0       750.0   1/7/2024     West\n",
      "4         5           Eve  Tablet       3.0       750.0   3/7/2024    South\n",
      "\n",
      "Before Removing Duplicates:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region\n",
      "0       101         Alice  Laptop       1.5       900.0   5/9/2024  Central\n",
      "1       102       Unknown  Laptop       1.0       300.0   5/7/2024  Central\n",
      "2       103       Unknown  Laptop       1.0       600.0   5/4/2024  Central\n",
      "3       104       Unknown  Tablet       1.5       300.0  5/26/2024  Central\n",
      "4       105         Heidi  Tablet       2.0       600.0  5/21/2024    North\n",
      "\n",
      "After Removing Duplicates:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region\n",
      "0       101         Alice  Laptop       1.5       900.0   5/9/2024  Central\n",
      "1       102       Unknown  Laptop       1.0       300.0   5/7/2024  Central\n",
      "2       103       Unknown  Laptop       1.0       600.0   5/4/2024  Central\n",
      "3       104       Unknown  Tablet       1.5       300.0  5/26/2024  Central\n",
      "4       105         Heidi  Tablet       2.0       600.0  5/21/2024    North\n",
      "Why: Removed 1 duplicates from raw_data and 0 from incremental_data to ensure unique records.\n",
      "\n",
      "Transformation 3: Adding total_price column\n",
      "\n",
      "Before Adding total_price:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region\n",
      "0         1         Diana  Tablet       2.0       500.0  1/20/2024    South\n",
      "1         2           Eve  Laptop       2.0       750.0  4/29/2024    North\n",
      "2         3       Charlie  Laptop       2.0       250.0   1/8/2024  Unknown\n",
      "3         4           Eve  Laptop       2.0       750.0   1/7/2024     West\n",
      "4         5           Eve  Tablet       3.0       750.0   3/7/2024    South\n",
      "\n",
      "After Adding total_price:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region  \\\n",
      "0         1         Diana  Tablet       2.0       500.0  1/20/2024    South   \n",
      "1         2           Eve  Laptop       2.0       750.0  4/29/2024    North   \n",
      "2         3       Charlie  Laptop       2.0       250.0   1/8/2024  Unknown   \n",
      "3         4           Eve  Laptop       2.0       750.0   1/7/2024     West   \n",
      "4         5           Eve  Tablet       3.0       750.0   3/7/2024    South   \n",
      "\n",
      "   total_price  \n",
      "0       1000.0  \n",
      "1       1500.0  \n",
      "2        500.0  \n",
      "3       1500.0  \n",
      "4       2250.0  \n",
      "\n",
      "Before Adding total_price:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region\n",
      "0       101         Alice  Laptop       1.5       900.0   5/9/2024  Central\n",
      "1       102       Unknown  Laptop       1.0       300.0   5/7/2024  Central\n",
      "2       103       Unknown  Laptop       1.0       600.0   5/4/2024  Central\n",
      "3       104       Unknown  Tablet       1.5       300.0  5/26/2024  Central\n",
      "4       105         Heidi  Tablet       2.0       600.0  5/21/2024    North\n",
      "\n",
      "After Adding total_price:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region  \\\n",
      "0       101         Alice  Laptop       1.5       900.0   5/9/2024  Central   \n",
      "1       102       Unknown  Laptop       1.0       300.0   5/7/2024  Central   \n",
      "2       103       Unknown  Laptop       1.0       600.0   5/4/2024  Central   \n",
      "3       104       Unknown  Tablet       1.5       300.0  5/26/2024  Central   \n",
      "4       105         Heidi  Tablet       2.0       600.0  5/21/2024    North   \n",
      "\n",
      "   total_price  \n",
      "0       1350.0  \n",
      "1        300.0  \n",
      "2        600.0  \n",
      "3        450.0  \n",
      "4       1200.0  \n",
      "Why: Added total_price (quantity * unit_price) to enable analysis of order value.\n",
      "\n",
      "Transformation 4: Converting order_date to datetime and extracting year\n",
      "\n",
      "Before Converting order_date:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region  \\\n",
      "0         1         Diana  Tablet       2.0       500.0  1/20/2024    South   \n",
      "1         2           Eve  Laptop       2.0       750.0  4/29/2024    North   \n",
      "2         3       Charlie  Laptop       2.0       250.0   1/8/2024  Unknown   \n",
      "3         4           Eve  Laptop       2.0       750.0   1/7/2024     West   \n",
      "4         5           Eve  Tablet       3.0       750.0   3/7/2024    South   \n",
      "\n",
      "   total_price  \n",
      "0       1000.0  \n",
      "1       1500.0  \n",
      "2        500.0  \n",
      "3       1500.0  \n",
      "4       2250.0  \n",
      "\n",
      "After Converting order_date:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region  \\\n",
      "0         1         Diana  Tablet       2.0       500.0 2024-01-20    South   \n",
      "1         2           Eve  Laptop       2.0       750.0 2024-04-29    North   \n",
      "2         3       Charlie  Laptop       2.0       250.0 2024-01-08  Unknown   \n",
      "3         4           Eve  Laptop       2.0       750.0 2024-01-07     West   \n",
      "4         5           Eve  Tablet       3.0       750.0 2024-03-07    South   \n",
      "\n",
      "   total_price  order_year  \n",
      "0       1000.0        2024  \n",
      "1       1500.0        2024  \n",
      "2        500.0        2024  \n",
      "3       1500.0        2024  \n",
      "4       2250.0        2024  \n",
      "\n",
      "Before Converting order_date:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region  \\\n",
      "0       101         Alice  Laptop       1.5       900.0   5/9/2024  Central   \n",
      "1       102       Unknown  Laptop       1.0       300.0   5/7/2024  Central   \n",
      "2       103       Unknown  Laptop       1.0       600.0   5/4/2024  Central   \n",
      "3       104       Unknown  Tablet       1.5       300.0  5/26/2024  Central   \n",
      "4       105         Heidi  Tablet       2.0       600.0  5/21/2024    North   \n",
      "\n",
      "   total_price  \n",
      "0       1350.0  \n",
      "1        300.0  \n",
      "2        600.0  \n",
      "3        450.0  \n",
      "4       1200.0  \n",
      "\n",
      "After Converting order_date:\n",
      "   order_id customer_name product  quantity  unit_price order_date   region  \\\n",
      "0       101         Alice  Laptop       1.5       900.0 2024-05-09  Central   \n",
      "1       102       Unknown  Laptop       1.0       300.0 2024-05-07  Central   \n",
      "2       103       Unknown  Laptop       1.0       600.0 2024-05-04  Central   \n",
      "3       104       Unknown  Tablet       1.5       300.0 2024-05-26  Central   \n",
      "4       105         Heidi  Tablet       2.0       600.0 2024-05-21    North   \n",
      "\n",
      "   total_price  order_year  \n",
      "0       1350.0        2024  \n",
      "1        300.0        2024  \n",
      "2        600.0        2024  \n",
      "3        450.0        2024  \n",
      "4       1200.0        2024  \n",
      "Why: Converted order_date to datetime with MM/DD/YYYY format for consistency and extracted year for time-based analysis.\n",
      "\n",
      "Transformed files saved to data/transformed/ directory.\n",
      "\n",
      "Bonus: Visualizing total sales by product\n",
      "Why: Visualized total sales by product to identify top-performing products. Saved as total_sales_by_product.png.\n"
     ]
    }
   ],
   "source": [
    "# etl_transform.py\n",
    "# This script applies at least four meaningful transformations to raw_data.csv and incremental_data.csv,\n",
    "# saves the results as transformed_full.csv and transformed_incremental.csv, and includes a visualization.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create data directory and save raw data if not already present\n",
    "data_path = 'data'\n",
    "if os.path.exists(data_path) and not os.path.isdir(data_path):\n",
    "    print(f\"Error: '{data_path}' exists as a file, not a directory. Please remove or rename the file.\")\n",
    "    raise FileExistsError(f\"Cannot create directory '{data_path}' because a file with that name exists.\")\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Create raw_data.csv and incremental_data.csv (using provided data structure)\n",
    "raw_data = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3, 4, 5, 4, 7, 8, 9, 10],  # Truncated for brevity, replace with full 100 rows if needed\n",
    "    'customer_name': ['Diana', 'Eve', 'Charlie', 'Eve', 'Eve', 'Eve', 'Charlie', 'Charlie', 'Charlie', 'Eve'],\n",
    "    'product': ['Tablet', 'Laptop', 'Laptop', 'Laptop', 'Tablet', 'Laptop', 'Monitor', 'Laptop', 'Monitor', 'Monitor'],\n",
    "    'quantity': [None, None, 2, 2, 3, 2, 2, 3, None, 1],\n",
    "    'unit_price': [500, None, 250, 750, None, 750, 750, None, 750, 500],\n",
    "    'order_date': ['1/20/2024', '4/29/2024', '1/8/2024', '1/7/2024', '3/7/2024', '1/7/2024', '2/2/2024', '2/17/2024', '3/16/2024', '2/28/2024'],\n",
    "    'region': ['South', 'North', None, 'West', 'South', 'West', 'West', None, 'West', 'North']\n",
    "})\n",
    "incremental_data = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    'customer_name': ['Alice', None, None, None, 'Heidi', None, None, None, 'Grace', 'Heidi'],\n",
    "    'product': ['Laptop', 'Laptop', 'Laptop', 'Tablet', 'Tablet', 'Laptop', 'Tablet', 'Laptop', 'Laptop', 'Phone'],\n",
    "    'quantity': [None, 1, 1, None, 2, 2, 1, None, 2, None],\n",
    "    'unit_price': [900, 300, 600, 300, 600, 600, 600, 600, 600, None],\n",
    "    'order_date': ['5/9/2024', '5/7/2024', '5/4/2024', '5/26/2024', '5/21/2024', '5/18/2024', '5/13/2024', '5/11/2024', '5/29/2024', '5/24/2024'],\n",
    "    'region': ['Central', 'Central', 'Central', 'Central', 'North', 'Central', 'Central', None, 'Central', None]\n",
    "})\n",
    "\n",
    "# Save raw data to CSV\n",
    "raw_data.to_csv('data/raw_data.csv', index=False)\n",
    "incremental_data.to_csv('data/incremental_data.csv', index=False)\n",
    "print(\"CSV files created in data/ folder\")\n",
    "\n",
    "# Create transformed directory if it doesn't exist\n",
    "transformed_path = 'data/transformed'\n",
    "if os.path.exists(transformed_path) and not os.path.isdir(transformed_path):\n",
    "    print(f\"Error: '{transformed_path}' exists as a file, not a directory. Please remove or rename the file.\")\n",
    "    raise FileExistsError(f\"Cannot create directory '{transformed_path}' because a file with that name exists.\")\n",
    "os.makedirs(transformed_path, exist_ok=True)\n",
    "\n",
    "# Load raw data\n",
    "raw_data = pd.read_csv('data/raw_data.csv')\n",
    "incremental_data = pd.read_csv('data/incremental_data.csv')\n",
    "\n",
    "# Function to display before and after\n",
    "def show_before_after(df_before, df_after, transformation_name):\n",
    "    print(f'\\nBefore {transformation_name}:')\n",
    "    print(df_before.head())\n",
    "    print(f'\\nAfter {transformation_name}:')\n",
    "    print(df_after.head())\n",
    "\n",
    "# Transformation 1: Cleaning - Handle missing values\n",
    "print('Transformation 1: Handling missing values')\n",
    "raw_data_cleaned = raw_data.copy()\n",
    "incremental_data_cleaned = incremental_data.copy()\n",
    "\n",
    "# Fill missing customer_name with 'Unknown'\n",
    "raw_data_cleaned['customer_name'] = raw_data_cleaned['customer_name'].fillna('Unknown')\n",
    "incremental_data_cleaned['customer_name'] = incremental_data_cleaned['customer_name'].fillna('Unknown')\n",
    "\n",
    "# Fill missing quantity with median\n",
    "raw_data_cleaned['quantity'] = raw_data_cleaned['quantity'].fillna(raw_data_cleaned['quantity'].median())\n",
    "incremental_data_cleaned['quantity'] = incremental_data_cleaned['quantity'].fillna(incremental_data_cleaned['quantity'].median())\n",
    "\n",
    "# Fill missing unit_price with median\n",
    "raw_data_cleaned['unit_price'] = raw_data_cleaned['unit_price'].fillna(raw_data_cleaned['unit_price'].median())\n",
    "incremental_data_cleaned['unit_price'] = incremental_data_cleaned['unit_price'].fillna(incremental_data_cleaned['unit_price'].median())\n",
    "\n",
    "# Fill missing region with 'Unknown'\n",
    "raw_data_cleaned['region'] = raw_data_cleaned['region'].fillna('Unknown')\n",
    "incremental_data_cleaned['region'] = incremental_data_cleaned['region'].fillna('Unknown')\n",
    "\n",
    "# Fill missing order_date with a default date\n",
    "raw_data_cleaned['order_date'] = raw_data_cleaned['order_date'].fillna('1/1/2024')\n",
    "incremental_data_cleaned['order_date'] = incremental_data_cleaned['order_date'].fillna('5/1/2024')\n",
    "\n",
    "show_before_after(raw_data, raw_data_cleaned, 'Cleaning - Missing Values')\n",
    "show_before_after(incremental_data, incremental_data_cleaned, 'Cleaning - Missing Values')\n",
    "print('Why: Missing values can skew analysis. Imputed customer_name and region with \\'Unknown\\', quantity and unit_price with median, and order_date with a default date to maintain data integrity.')\n",
    "\n",
    "# Transformation 2: Cleaning - Remove duplicates\n",
    "print('\\nTransformation 2: Removing duplicates')\n",
    "raw_data_no_duplicates = raw_data_cleaned.drop_duplicates()\n",
    "incremental_data_no_duplicates = incremental_data_cleaned.drop_duplicates()\n",
    "\n",
    "show_before_after(raw_data_cleaned, raw_data_no_duplicates, 'Removing Duplicates')\n",
    "show_before_after(incremental_data_cleaned, incremental_data_no_duplicates, 'Removing Duplicates')\n",
    "print(f'Why: Removed {len(raw_data_cleaned) - len(raw_data_no_duplicates)} duplicates from raw_data and {len(incremental_data_cleaned) - len(incremental_data_no_duplicates)} from incremental_data to ensure unique records.')\n",
    "\n",
    "# Transformation 3: Enrichment - Add total_price column\n",
    "print('\\nTransformation 3: Adding total_price column')\n",
    "raw_data_enriched = raw_data_no_duplicates.copy()\n",
    "incremental_data_enriched = incremental_data_no_duplicates.copy()\n",
    "\n",
    "raw_data_enriched['total_price'] = raw_data_enriched['quantity'] * raw_data_enriched['unit_price']\n",
    "incremental_data_enriched['total_price'] = incremental_data_enriched['quantity'] * incremental_data_enriched['unit_price']\n",
    "\n",
    "show_before_after(raw_data_no_duplicates, raw_data_enriched, 'Adding total_price')\n",
    "show_before_after(incremental_data_no_duplicates, incremental_data_enriched, 'Adding total_price')\n",
    "print('Why: Added total_price (quantity * unit_price) to enable analysis of order value.')\n",
    "\n",
    "# Transformation 4: Structural - Convert order_date to datetime and extract year\n",
    "print('\\nTransformation 4: Converting order_date to datetime and extracting year')\n",
    "raw_data_structured = raw_data_enriched.copy()\n",
    "incremental_data_structured = incremental_data_enriched.copy()\n",
    "\n",
    "# Convert order_date to datetime, handling MM/DD/YYYY format\n",
    "raw_data_structured['order_date'] = pd.to_datetime(raw_data_structured['order_date'], format='%m/%d/%Y')\n",
    "incremental_data_structured['order_date'] = pd.to_datetime(incremental_data_structured['order_date'], format='%m/%d/%Y')\n",
    "\n",
    "raw_data_structured['order_year'] = raw_data_structured['order_date'].dt.year\n",
    "incremental_data_structured['order_year'] = incremental_data_structured['order_date'].dt.year\n",
    "\n",
    "show_before_after(raw_data_enriched, raw_data_structured, 'Converting order_date')\n",
    "show_before_after(incremental_data_enriched, incremental_data_structured, 'Converting order_date')\n",
    "print('Why: Converted order_date to datetime with MM/DD/YYYY format for consistency and extracted year for time-based analysis.')\n",
    "\n",
    "# Save transformed files\n",
    "raw_data_structured.to_csv('data/transformed/transformed_full.csv', index=False)\n",
    "incremental_data_structured.to_csv('data/transformed/transformed_incremental.csv', index=False)\n",
    "print('\\nTransformed files saved to data/transformed/ directory.')\n",
    "\n",
    "# Bonus: Visualization - Bar chart of total sales by product\n",
    "print('\\nBonus: Visualizing total sales by product')\n",
    "total_sales = raw_data_structured.groupby('product')['total_price'].sum().reset_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(total_sales['product'], total_sales['total_price'], color='skyblue')\n",
    "plt.title('Total Sales by Product')\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('Total Sales ($)')\n",
    "plt.savefig('total_sales_by_product.png')\n",
    "plt.close()\n",
    "print('Why: Visualized total sales by product to identify top-performing products. Saved as total_sales_by_product.png.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
